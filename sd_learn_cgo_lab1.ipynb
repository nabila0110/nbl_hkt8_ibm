{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "urkECkXbAgo6",
        "outputId": "febada47-36e8-4264-96d4-b99baf147bf9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/ibm-granite-community/utils\n",
            "  Cloning https://github.com/ibm-granite-community/utils to /tmp/pip-req-build-1kr59x4a\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/ibm-granite-community/utils /tmp/pip-req-build-1kr59x4a\n",
            "  Resolved https://github.com/ibm-granite-community/utils to commit 1514191fbbc4605ed4fdfdcb448f2ee41477058f\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: langchain_community<0.3.0 in /usr/local/lib/python3.11/dist-packages (0.2.19)\n",
            "Requirement already satisfied: replicate in /usr/local/lib/python3.11/dist-packages (1.0.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (from ibm-granite-community-utils==0.1.dev74) (1.1.1)\n",
            "Requirement already satisfied: langchain_core in /usr/local/lib/python3.11/dist-packages (from ibm-granite-community-utils==0.1.dev74) (0.2.43)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from ibm-granite-community-utils==0.1.dev74) (4.14.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community<0.3.0) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community<0.3.0) (2.0.41)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community<0.3.0) (3.11.15)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain_community<0.3.0) (0.6.7)\n",
            "Requirement already satisfied: langchain<0.3.0,>=0.2.17 in /usr/local/lib/python3.11/dist-packages (from langchain_community<0.3.0) (0.2.17)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.112 in /usr/local/lib/python3.11/dist-packages (from langchain_community<0.3.0) (0.1.147)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.11/dist-packages (from langchain_community<0.3.0) (1.26.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community<0.3.0) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community<0.3.0) (8.5.0)\n",
            "Requirement already satisfied: httpx<1,>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from replicate) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from replicate) (24.2)\n",
            "Requirement already satisfied: pydantic>1.10.7 in /usr/local/lib/python3.11/dist-packages (from replicate) (2.11.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community<0.3.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community<0.3.0) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community<0.3.0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community<0.3.0) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community<0.3.0) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community<0.3.0) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community<0.3.0) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community<0.3.0) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community<0.3.0) (0.9.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.21.0->replicate) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.21.0->replicate) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.21.0->replicate) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.21.0->replicate) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.21.0->replicate) (0.16.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from langchain<0.3.0,>=0.2.17->langchain_community<0.3.0) (0.2.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain_core->ibm-granite-community-utils==0.1.dev74) (1.33)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.112->langchain_community<0.3.0) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.112->langchain_community<0.3.0) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>1.10.7->replicate) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>1.10.7->replicate) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>1.10.7->replicate) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community<0.3.0) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community<0.3.0) (2.4.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community<0.3.0) (3.2.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain_core->ibm-granite-community-utils==0.1.dev74) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community<0.3.0) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.21.0->replicate) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/ibm-granite-community/utils \\\n",
        "    \"langchain_community<0.3.0\" \\\n",
        "    replicate \\\n",
        "    pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3r0HxqRQNo6A",
        "outputId": "ba2325dc-29c9-49f2-cbff-368ac1425a60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "REPLICATE_API_TOKEN loaded from Google Colab secret.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML\n",
        "from ibm_granite_community.notebook_utils import get_env_var\n",
        "from langchain_community.llms import Replicate\n",
        "\n",
        "# Initialize the model\n",
        "model = Replicate(\n",
        "    model=\"ibm-granite/granite-3.3-8b-instruct\",\n",
        "    replicate_api_token=get_env_var('REPLICATE_API_TOKEN'),\n",
        "    model_kwargs={\"max_tokens\":1024, \"temperature\":0.2},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3URKgq3vOME_"
      },
      "outputs": [],
      "source": [
        "class AcademicAIAgent:\n",
        "    def __init__(self):\n",
        "        self.papers_data = []\n",
        "        self.load_sample_data()\n",
        "\n",
        "    def load_sample_data(self):\n",
        "        \"\"\"Load sample academic papers data\"\"\"\n",
        "        # Load data from your CSV and JSON files\n",
        "        try:\n",
        "            # Load CSV file\n",
        "            df = pd.read_csv('../indonesian_cs_papers.csv')\n",
        "            self.papers_data = self.process_csv_data(df)\n",
        "            print(f\"✅ Loaded {len(self.papers_data)} papers from CSV\")\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(\"⚠️ File indonesian_cs_papers.csv tidak ditemukan, menggunakan sample data\")\n",
        "            # Fallback sample data\n",
        "            self.papers_data = [\n",
        "                {\n",
        "                    \"title\": \"Machine Learning Klasifikasi Status Gizi Balita Menggunakan Algoritma Random Forest\",\n",
        "                    \"authors\": [\"Handayani, P.\", \"Charis Fauzan, A.\", \"Harlina\"],\n",
        "                    \"year\": 2024,\n",
        "                    \"journal\": \"KLIK: Kajian Ilmiah Informatika Dan Komputer\",\n",
        "                    \"abstract\": \"Penelitian ini menggunakan algoritma Random Forest untuk klasifikasi status gizi balita...\",\n",
        "                    \"keywords\": [\"machine learning\", \"random forest\", \"status gizi\", \"balita\"],\n",
        "                    \"content\": \"Penelitian ini bertujuan untuk mengklasifikasikan status gizi balita menggunakan algoritma Random Forest...\",\n",
        "                    \"doi\": \"10.30865/klik.v4i6.1909\",\n",
        "                    \"url\": \"\",\n",
        "                    \"citation\": \"Handayani, P., Charis Fauzan, A., & Harlina. (2024). Machine Learning Klasifikasi Status Gizi Balita Menggunakan Algoritma Random Forest. KLIK: Kajian Ilmiah Informatika Dan Komputer, 4(6), 3064–3072.\"\n",
        "                }\n",
        "            ]\n",
        "\n",
        "        try:\n",
        "            # Also try to load JSON file if available\n",
        "            with open('../indonesian_cs_papers.json', 'r', encoding='utf-8') as f:\n",
        "                json_data = json.load(f)\n",
        "                additional_data = self.process_json_data(json_data)\n",
        "                self.papers_data.extend(additional_data)\n",
        "                print(f\"✅ Loaded additional {len(additional_data)} papers from JSON\")\n",
        "        except FileNotFoundError:\n",
        "            print(\"⚠️ File indonesian_cs_papers.json tidak ditemukan, hanya menggunakan data CSV\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Error loading JSON: {e}\")\n",
        "\n",
        "    def process_csv_data(self, df):\n",
        "        \"\"\"Process CSV data to match internal structure\"\"\"\n",
        "        processed_data = []\n",
        "\n",
        "        for _, row in df.iterrows():\n",
        "            # Extract keywords from search_term if available\n",
        "            keywords = []\n",
        "            if pd.notna(row.get('search_term')):\n",
        "                keywords = [term.strip() for term in str(row['search_term']).split(',')]\n",
        "\n",
        "            # Process authors\n",
        "            authors = []\n",
        "            if pd.notna(row.get('authors')):\n",
        "                authors = [auth.strip() for auth in str(row['authors']).split(';')]\n",
        "            elif pd.notna(row.get('authors_raw')):\n",
        "                authors = [auth.strip() for auth in str(row['authors_raw']).split(',')]\n",
        "\n",
        "            paper = {\n",
        "                \"title\": str(row.get('title', '')),\n",
        "                \"authors\": authors,\n",
        "                \"year\": int(row.get('year', 0)) if pd.notna(row.get('year')) else 0,\n",
        "                \"journal\": str(row.get('journal', '')),\n",
        "                \"abstract\": str(row.get('abstract', '')),\n",
        "                \"keywords\": keywords,\n",
        "                \"content\": str(row.get('abstract', '')),  # Using abstract as content for now\n",
        "                \"doi\": str(row.get('doi', '')),\n",
        "                \"url\": str(row.get('url', '')),\n",
        "                \"pdf_url\": str(row.get('pdf_url', '')),\n",
        "                \"citation\": str(row.get('citation', '')),\n",
        "                \"citation_count\": int(row.get('citation_count', 0)) if pd.notna(row.get('citation_count')) else 0,\n",
        "                \"categories\": str(row.get('categories', '')),\n",
        "                \"published_date\": str(row.get('published_date', ''))\n",
        "            }\n",
        "            processed_data.append(paper)\n",
        "\n",
        "        return processed_data\n",
        "\n",
        "    def process_json_data(self, json_data):\n",
        "        \"\"\"Process JSON data to match internal structure\"\"\"\n",
        "        processed_data = []\n",
        "\n",
        "        for item in json_data:\n",
        "            # Extract keywords\n",
        "            keywords = []\n",
        "            if 'search_term' in item:\n",
        "                keywords = [term.strip() for term in str(item['search_term']).split(',')]\n",
        "\n",
        "            # Process authors\n",
        "            authors = []\n",
        "            if 'authors' in item:\n",
        "                authors = [auth.strip() for auth in str(item['authors']).split(';')]\n",
        "            elif 'authors_raw' in item:\n",
        "                authors = [auth.strip() for auth in str(item['authors_raw']).split(',')]\n",
        "\n",
        "            paper = {\n",
        "                \"title\": str(item.get('title', '')),\n",
        "                \"authors\": authors,\n",
        "                \"year\": int(item.get('year', 0)) if item.get('year') else 0,\n",
        "                \"journal\": str(item.get('journal', '')),\n",
        "                \"abstract\": str(item.get('abstract', '')),\n",
        "                \"keywords\": keywords,\n",
        "                \"content\": str(item.get('abstract', '')),\n",
        "                \"doi\": str(item.get('doi', '')),\n",
        "                \"url\": str(item.get('url', '')),\n",
        "                \"pdf_url\": str(item.get('pdf_url', '')),\n",
        "                \"citation\": str(item.get('citation', '')),\n",
        "                \"citation_count\": int(item.get('citation_count', 0)) if item.get('citation_count') else 0,\n",
        "                \"categories\": str(item.get('categories', '')),\n",
        "                \"published_date\": str(item.get('published_date', ''))\n",
        "            }\n",
        "            processed_data.append(paper)\n",
        "\n",
        "        return processed_data\n",
        "\n",
        "    def load_data_from_files(self, csv_file=None, json_file=None):\n",
        "        \"\"\"Load data from CSV or JSON files\"\"\"\n",
        "        if csv_file:\n",
        "            try:\n",
        "                df = pd.read_csv(csv_file)\n",
        "                self.papers_data.extend(df.to_dict('records'))\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading CSV: {e}\")\n",
        "\n",
        "        if json_file:\n",
        "            try:\n",
        "                with open(json_file, 'r') as f:\n",
        "                    data = json.load(f)\n",
        "                    self.papers_data.extend(data)\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading JSON: {e}\")\n",
        "\n",
        "    def search_papers(self, query):\n",
        "        \"\"\"Search papers based on query\"\"\"\n",
        "        # Enhanced search implementation for your data structure\n",
        "        results = []\n",
        "        query_lower = query.lower()\n",
        "\n",
        "        for paper in self.papers_data:\n",
        "            score = 0\n",
        "\n",
        "            # Search in title (highest priority)\n",
        "            if query_lower in paper['title'].lower():\n",
        "                score += 10\n",
        "\n",
        "            # Search in abstract (medium priority)\n",
        "            if query_lower in paper['abstract'].lower():\n",
        "                score += 5\n",
        "\n",
        "            # Search in keywords (medium priority)\n",
        "            if paper['keywords']:\n",
        "                for keyword in paper['keywords']:\n",
        "                    if query_lower in keyword.lower() or keyword.lower() in query_lower:\n",
        "                        score += 3\n",
        "\n",
        "            # Search in journal (low priority)\n",
        "            if query_lower in paper['journal'].lower():\n",
        "                score += 2\n",
        "\n",
        "            # Search in categories (low priority)\n",
        "            if query_lower in paper['categories'].lower():\n",
        "                score += 2\n",
        "\n",
        "            # Search in authors (low priority)\n",
        "            if paper['authors']:\n",
        "                for author in paper['authors']:\n",
        "                    if query_lower in author.lower():\n",
        "                        score += 1\n",
        "\n",
        "            if score > 0:\n",
        "                paper['relevance_score'] = score\n",
        "                results.append(paper)\n",
        "\n",
        "        # Sort by relevance score (descending)\n",
        "        results.sort(key=lambda x: x.get('relevance_score', 0), reverse=True)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def format_citation(self, paper):\n",
        "        \"\"\"Format citation in APA style\"\"\"\n",
        "        # Use existing citation if available, otherwise create new one\n",
        "        if paper.get('citation') and paper['citation'].strip():\n",
        "            return paper['citation']\n",
        "\n",
        "        # Create citation from available data\n",
        "        if paper.get('authors') and len(paper['authors']) > 0:\n",
        "            if len(paper['authors']) == 1:\n",
        "                authors = paper['authors'][0]\n",
        "            elif len(paper['authors']) == 2:\n",
        "                authors = f\"{paper['authors'][0]} & {paper['authors'][1]}\"\n",
        "            else:\n",
        "                authors = f\"{paper['authors'][0]} et al.\"\n",
        "        else:\n",
        "            authors = \"Unknown Author\"\n",
        "\n",
        "        citation = f\"{authors} ({paper.get('year', 'n.d.')}). {paper.get('title', 'Untitled')}. \"\n",
        "\n",
        "        if paper.get('journal'):\n",
        "            citation += f\"*{paper['journal']}*. \"\n",
        "\n",
        "        if paper.get('url'):\n",
        "            citation += f\"{paper['url']}\"\n",
        "        elif paper.get('doi'):\n",
        "            citation += f\"https://doi.org/{paper['doi']}\"\n",
        "\n",
        "        return citation\n",
        "\n",
        "    def generate_smart_answer(self, query, relevant_papers):\n",
        "        \"\"\"Generate AI-powered answer based on relevant papers\"\"\"\n",
        "        if not relevant_papers:\n",
        "            return \"Tidak ditemukan paper yang relevan dengan pertanyaan Anda.\"\n",
        "\n",
        "        # Create context from relevant papers\n",
        "        context = \"\"\n",
        "        for paper in relevant_papers[:3]:  # Use top 3 relevant papers\n",
        "            context += f\"Paper: {paper['title']}\\n\"\n",
        "            context += f\"Abstract: {paper['abstract']}\\n\"\n",
        "            context += f\"Content: {paper['content'][:200]}...\\n\\n\"\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        Sebagai AI assistant akademik, jawab pertanyaan berikut berdasarkan paper-paper yang relevan:\n",
        "\n",
        "        Pertanyaan: {query}\n",
        "\n",
        "        Konteks dari paper-paper relevan:\n",
        "        {context}\n",
        "\n",
        "        Berikan jawaban yang informatif dan akademis, serta sebutkan sumber yang mendukung jawaban Anda.\n",
        "        Jawaban harus dalam bahasa Indonesia dan bersifat objektif.\n",
        "        \"\"\"\n",
        "\n",
        "        try:\n",
        "            answer = model.invoke(prompt)\n",
        "            return answer\n",
        "        except Exception as e:\n",
        "            return f\"Error generating answer: {e}\"\n",
        "\n",
        "    def generate_paper(self, title=\"\", abstract=\"\", method=\"\", custom_prompt=\"\"):\n",
        "        \"\"\"Generate a complete academic paper\"\"\"\n",
        "        if custom_prompt:\n",
        "            prompt = f\"\"\"\n",
        "            Sebagai AI assistant akademik, buatkan draft paper penelitian berdasarkan prompt berikut:\n",
        "            {custom_prompt}\n",
        "\n",
        "            Format paper harus mencakup:\n",
        "            1. Judul\n",
        "            2. Abstract\n",
        "            3. Pendahuluan\n",
        "            4. Metodologi\n",
        "            5. Hasil dan Pembahasan\n",
        "            6. Kesimpulan\n",
        "            7. Daftar Pustaka (minimal 5 referensi)\n",
        "\n",
        "            Pastikan konten akademis, objektif, dan mengikuti standar penulisan ilmiah.\n",
        "            \"\"\"\n",
        "        else:\n",
        "            prompt = f\"\"\"\n",
        "            Sebagai AI assistant akademik, buatkan draft paper penelitian dengan informasi berikut:\n",
        "\n",
        "            Judul: {title if title else \"Belum ditentukan\"}\n",
        "            Abstract: {abstract if abstract else \"Belum ditentukan\"}\n",
        "            Metodologi: {method if method else \"Belum ditentukan\"}\n",
        "\n",
        "            Lengkapi dan kembangkan paper dengan struktur:\n",
        "            1. Judul (jika belum ada)\n",
        "            2. Abstract (jika belum lengkap)\n",
        "            3. Pendahuluan\n",
        "            4. Metodologi (jika belum lengkap)\n",
        "            5. Hasil dan Pembahasan\n",
        "            6. Kesimpulan\n",
        "            7. Daftar Pustaka\n",
        "\n",
        "            Pastikan konten akademis, objektif, dan mengikuti standar penulisan ilmiah.\n",
        "            \"\"\"\n",
        "\n",
        "        try:\n",
        "            paper = model.invoke(prompt)\n",
        "            return paper\n",
        "        except Exception as e:\n",
        "            return f\"Error generating paper: {e}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "f-o9FFvbOQQJ",
        "outputId": "a29c3980-7d87-4e3d-b02e-6c50777dae83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ File indonesian_cs_papers.csv tidak ditemukan, menggunakan sample data\n",
            "⚠️ File indonesian_cs_papers.json tidak ditemukan, hanya menggunakan data CSV\n"
          ]
        }
      ],
      "source": [
        "# Initialize the AI Agent\n",
        "agent = AcademicAIAgent()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ScSaBu9OXRI"
      },
      "outputs": [],
      "source": [
        "# Create UI Components\n",
        "def create_search_ui():\n",
        "    \"\"\"Create search interface\"\"\"\n",
        "    search_input = widgets.Text(\n",
        "        placeholder=\"Masukkan pertanyaan atau topik penelitian...\",\n",
        "        style={'description_width': 'initial'},\n",
        "        layout=widgets.Layout(width='80%')\n",
        "    )\n",
        "\n",
        "    search_button = widgets.Button(\n",
        "        description=\"Cari\",\n",
        "        button_style='primary',\n",
        "        layout=widgets.Layout(width='15%')\n",
        "    )\n",
        "\n",
        "    search_output = widgets.Output()\n",
        "\n",
        "    def on_search_click(b):\n",
        "        with search_output:\n",
        "            search_output.clear_output()\n",
        "            query = search_input.value\n",
        "            if query:\n",
        "                print(\"🔍 Mencari paper yang relevan...\")\n",
        "                relevant_papers = agent.search_papers(query)\n",
        "\n",
        "                if relevant_papers:\n",
        "                    print(f\"📚 Ditemukan {len(relevant_papers)} paper relevan\\n\")\n",
        "\n",
        "                    # Generate smart answer\n",
        "                    print(\"🤖 Generating AI Answer...\")\n",
        "                    answer = agent.generate_smart_answer(query, relevant_papers)\n",
        "                    print(f\"Jawaban AI:\\n{answer}\\n\")\n",
        "\n",
        "                    print(\"📖 Sumber Referensi:\")\n",
        "                    for i, paper in enumerate(relevant_papers[:5], 1):\n",
        "                        citation = agent.format_citation(paper)\n",
        "                        print(f\"{i}. {citation}\")\n",
        "\n",
        "                        # Show additional info if available\n",
        "                        if paper.get('citation_count', 0) > 0:\n",
        "                            print(f\"   📊 Citations: {paper['citation_count']}\")\n",
        "                        if paper.get('pdf_url'):\n",
        "                            print(f\"   📄 PDF: {paper['pdf_url']}\")\n",
        "                        if paper.get('categories'):\n",
        "                            print(f\"   🏷️ Categories: {paper['categories']}\")\n",
        "                        print()\n",
        "                else:\n",
        "                    print(\"❌ Tidak ditemukan paper yang relevan\")\n",
        "\n",
        "    search_button.on_click(on_search_click)\n",
        "\n",
        "    return widgets.VBox([\n",
        "        widgets.HTML(\"<h3>🔍 Pencarian Pintar Paper Akademik</h3>\"),\n",
        "        widgets.HBox([search_input, search_button]),\n",
        "        search_output\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rq2u2djTsdK"
      },
      "outputs": [],
      "source": [
        "def create_generator_ui():\n",
        "    \"\"\"Create paper generator interface\"\"\"\n",
        "    title_input = widgets.Text(\n",
        "        description=\"Judul:\",\n",
        "        placeholder=\"Masukkan judul penelitian (opsional)\",\n",
        "        style={'description_width': 'initial'},\n",
        "        layout=widgets.Layout(width='100%')\n",
        "    )\n",
        "\n",
        "    abstract_input = widgets.Textarea(\n",
        "        description=\"Abstract:\",\n",
        "        placeholder=\"Masukkan abstract (opsional)\",\n",
        "        style={'description_width': 'initial'},\n",
        "        layout=widgets.Layout(width='100%', height='80px')\n",
        "    )\n",
        "\n",
        "    method_input = widgets.Textarea(\n",
        "        description=\"Metodologi:\",\n",
        "        placeholder=\"Masukkan metodologi penelitian (opsional)\",\n",
        "        style={'description_width': 'initial'},\n",
        "        layout=widgets.Layout(width='100%', height='80px')\n",
        "    )\n",
        "\n",
        "    prompt_input = widgets.Textarea(\n",
        "        description=\"Custom Prompt:\",\n",
        "        placeholder=\"Atau masukkan prompt khusus untuk jenis penelitian yang diinginkan\",\n",
        "        style={'description_width': 'initial'},\n",
        "        layout=widgets.Layout(width='100%', height='80px')\n",
        "    )\n",
        "\n",
        "    generate_button = widgets.Button(\n",
        "        description=\"Generate Paper\",\n",
        "        button_style='success',\n",
        "        layout=widgets.Layout(width='200px')\n",
        "    )\n",
        "\n",
        "    generator_output = widgets.Output()\n",
        "\n",
        "    def on_generate_click(b):\n",
        "        with generator_output:\n",
        "            generator_output.clear_output()\n",
        "            print(\"📝 Generating paper...\")\n",
        "\n",
        "            paper = agent.generate_paper(\n",
        "                title=title_input.value,\n",
        "                abstract=abstract_input.value,\n",
        "                method=method_input.value,\n",
        "                custom_prompt=prompt_input.value\n",
        "            )\n",
        "\n",
        "            print(\"📄 Generated Paper:\")\n",
        "            print(\"=\"*50)\n",
        "            print(paper)\n",
        "\n",
        "    generate_button.on_click(on_generate_click)\n",
        "\n",
        "    return widgets.VBox([\n",
        "        widgets.HTML(\"<h3>📝 Generator Paper Akademik</h3>\"),\n",
        "        title_input,\n",
        "        abstract_input,\n",
        "        method_input,\n",
        "        prompt_input,\n",
        "        generate_button,\n",
        "        generator_output\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_data_upload_ui():\n",
        "    \"\"\"Create data upload interface\"\"\"\n",
        "    upload_csv = widgets.FileUpload(\n",
        "        accept='.csv',\n",
        "        description='Upload CSV',\n",
        "        multiple=False\n",
        "    )\n",
        "\n",
        "    upload_json = widgets.FileUpload(\n",
        "        accept='.json',\n",
        "        description='Upload JSON',\n",
        "        multiple=False\n",
        "    )\n",
        "\n",
        "    # Button to load files from system\n",
        "    load_files_button = widgets.Button(\n",
        "        description=\"Load Files from System\",\n",
        "        button_style='info'\n",
        "    )\n",
        "\n",
        "    upload_output = widgets.Output()\n",
        "\n",
        "    def load_files_from_system(b):\n",
        "        \"\"\"Try to load files from system\"\"\"\n",
        "        with upload_output:\n",
        "            upload_output.clear_output()\n",
        "            print(\"🔍 Searching for files in system...\")\n",
        "\n",
        "            # Reinitialize agent to load files\n",
        "            global agent\n",
        "            agent = AcademicAIAgent()\n",
        "\n",
        "    def on_upload_csv_change(change):\n",
        "        \"\"\"Handle CSV file upload\"\"\"\n",
        "        with upload_output:\n",
        "            upload_output.clear_output()\n",
        "            if change['new']:\n",
        "                try:\n",
        "                    # Get the uploaded file\n",
        "                    uploaded_file = change['new'][0]\n",
        "                    content = uploaded_file['content']\n",
        "\n",
        "                    # Save file temporarily\n",
        "                    with open('temp_papers.csv', 'wb') as f:\n",
        "                        f.write(content)\n",
        "\n",
        "                    # Load the data\n",
        "                    df = pd.read_csv('temp_papers.csv')\n",
        "                    global agent\n",
        "                    agent.papers_data = agent.process_csv_data(df)\n",
        "\n",
        "                    print(f\"✅ Successfully loaded {len(agent.papers_data)} papers from uploaded CSV!\")\n",
        "                    print(\"📊 Sample data:\")\n",
        "                    if agent.papers_data:\n",
        "                        sample = agent.papers_data[0]\n",
        "                        print(f\"   Title: {sample.get('title', 'N/A')}\")\n",
        "                        print(f\"   Authors: {sample.get('authors', 'N/A')}\")\n",
        "                        print(f\"   Year: {sample.get('year', 'N/A')}\")\n",
        "                        print(f\"   Journal: {sample.get('journal', 'N/A')}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"❌ Error processing CSV file: {e}\")\n",
        "\n",
        "    def on_upload_json_change(change):\n",
        "        \"\"\"Handle JSON file upload\"\"\"\n",
        "        with upload_output:\n",
        "            upload_output.clear_output()\n",
        "            if change['new']:\n",
        "                try:\n",
        "                    # Get the uploaded file\n",
        "                    uploaded_file = change['new'][0]\n",
        "                    content = uploaded_file['content']\n",
        "\n",
        "                    # Save file temporarily\n",
        "                    with open('temp_papers.json', 'wb') as f:\n",
        "                        f.write(content)\n",
        "\n",
        "                    # Load the data\n",
        "                    with open('temp_papers.json', 'r', encoding='utf-8') as f:\n",
        "                        json_data = json.load(f)\n",
        "\n",
        "                    global agent\n",
        "                    additional_data = agent.process_json_data(json_data)\n",
        "\n",
        "                    if hasattr(agent, 'papers_data') and agent.papers_data:\n",
        "                        agent.papers_data.extend(additional_data)\n",
        "                        print(f\"✅ Added {len(additional_data)} papers from JSON!\")\n",
        "                    else:\n",
        "                        agent.papers_data = additional_data\n",
        "                        print(f\"✅ Loaded {len(additional_data)} papers from JSON!\")\n",
        "\n",
        "                    print(f\"📊 Total papers: {len(agent.papers_data)}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"❌ Error processing JSON file: {e}\")\n",
        "\n",
        "    upload_csv.observe(on_upload_csv_change, names='value')\n",
        "    upload_json.observe(on_upload_json_change, names='value')\n",
        "    load_files_button.on_click(load_files_from_system)\n",
        "\n",
        "    return widgets.VBox([\n",
        "        widgets.HTML(\"<h3>📁 Upload Data Paper</h3>\"),\n",
        "        widgets.HTML(\"<p>Option 1: Upload file CSV atau JSON secara manual</p>\"),\n",
        "        widgets.HBox([upload_csv, upload_json]),\n",
        "        widgets.HTML(\"<p>Option 2: Load files yang sudah ada di sistem</p>\"),\n",
        "        load_files_button,\n",
        "        upload_output\n",
        "    ])\n"
      ],
      "metadata": {
        "id": "mbV8jiN7iL1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create main application\n",
        "def create_main_app():\n",
        "    \"\"\"Create the main application\"\"\"\n",
        "    # Header\n",
        "    header = widgets.HTML(\"\"\"\n",
        "    <div style=\"text-align: center; padding: 20px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; border-radius: 10px; margin-bottom: 20px;\">\n",
        "        <h1>🤖 AI Agent untuk Karya Tulis Ilmiah</h1>\n",
        "        <p>Asisten AI untuk pencarian paper dan pembuatan karya tulis ilmiah</p>\n",
        "    </div>\n",
        "    \"\"\")\n",
        "\n",
        "    # Create tabs\n",
        "    tab = widgets.Tab()\n",
        "    tab.children = [\n",
        "        create_search_ui(),\n",
        "        create_generator_ui(),\n",
        "        create_data_upload_ui()\n",
        "    ]\n",
        "\n",
        "    tab.set_title(0, \"🔍 Pencarian Pintar\")\n",
        "    tab.set_title(1, \"📝 Generator Paper\")\n",
        "    tab.set_title(2, \"📁 Upload Data\")\n",
        "\n",
        "    # Footer\n",
        "    footer = widgets.HTML(\"\"\"\n",
        "    <div style=\"text-align: center; padding: 10px; color: #666; margin-top: 20px;\">\n",
        "        <p>Powered by IBM Granite & LangChain | Developed for Academic Research</p>\n",
        "    </div>\n",
        "    \"\"\")\n",
        "\n",
        "    return widgets.VBox([header, tab, footer])"
      ],
      "metadata": {
        "id": "SwL6WvWViQCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the application\n",
        "main_app = create_main_app()\n",
        "display(main_app)"
      ],
      "metadata": {
        "id": "It139_HMiT_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instructions for data loading\n",
        "print(\"\"\"\n",
        "📋 Petunjuk Penggunaan:\n",
        "\n",
        "1. 🔍 Pencarian Pintar:\n",
        "   - Masukkan pertanyaan atau topik penelitian\n",
        "   - Sistem akan mencari paper yang relevan dari database Indonesian CS Papers\n",
        "   - AI akan memberikan jawaban dengan sitasi otomatis\n",
        "   - Menampilkan citation count dan link PDF jika tersedia\n",
        "\n",
        "2. 📝 Generator Paper:\n",
        "   - Isi judul, abstract, metodologi, atau custom prompt\n",
        "   - Sistem akan generate paper lengkap berbasis CS Indonesia\n",
        "   - Hasil dapat digunakan sebagai draft awal\n",
        "\n",
        "3. 📁 Upload Data:\n",
        "   - Sistem otomatis memuat indonesian_cs_papers.csv dan indonesian_cs_papers.json\n",
        "   - Jika file tidak ditemukan, sistem akan menggunakan sample data\n",
        "\n",
        "📊 Data yang Dimuat:\n",
        "   - File CSV: indonesian_cs_papers.csv\n",
        "   - File JSON: indonesian_cs_papers.json\n",
        "   - Kolom utama: title, authors, abstract, year, journal, doi, citation, dll.\n",
        "\n",
        "💡 Tips:\n",
        "- Gunakan kata kunci spesifik seperti \"machine learning\", \"deep learning\", \"computer vision\"\n",
        "- Sistem akan memberikan relevance score untuk hasil pencarian\n",
        "- Citation count dan PDF link akan ditampilkan jika tersedia\n",
        "- Data fokus pada Computer Science papers dari Indonesia\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "V1Fu1CTbiV-p"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}